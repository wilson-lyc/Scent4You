{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a6b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92dceda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36877e",
   "metadata": {},
   "source": [
    "# Note Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97a4135d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "note_labels_num = 21\n",
    "note_tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "nem = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=note_labels_num).to(device)\n",
    "state_dict = torch.load(\"models/NEM_v1.pth\", map_location=device)\n",
    "nem.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fef1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary\n",
    "df_ne = pd.read_csv(\"data/note_embedding_v1.csv\")\n",
    "ne_list = df_ne.iloc[:, 1:].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf8c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def note2vec(note):\n",
    "    # 将香材文本转为向量\n",
    "    nem.eval()\n",
    "    with torch.no_grad():\n",
    "        encoding = note_tokenizer(\n",
    "            note,\n",
    "            truncation=True,\n",
    "            padding='max_length', \n",
    "            max_length=64,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        outputs = nem.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    \n",
    "    # 推荐近似香材\n",
    "    embedding = embedding.reshape(1, -1)\n",
    "    similarities = cosine_similarity(embedding, ne_list)[0]\n",
    "    idx = similarities.argmax()\n",
    "    rc_vec = ne_list[idx]\n",
    "    rc_name = df_ne.iloc[idx, 0]\n",
    "    rc_similarity = similarities[idx]\n",
    "    return rc_vec, rc_name, rc_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "690864cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_note_list_vec(note_list):\n",
    "    vec_list = []\n",
    "    for note in note_list:\n",
    "        vec, name, similarity = note2vec(note)\n",
    "        vec_list.append(vec)\n",
    "        print(f\"Note: {note} => {name} (similarity: {similarity:.4f})\")\n",
    "    return vec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b6113",
   "metadata": {},
   "source": [
    "# Perfume Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747db24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "class PerfumeEmbedding(nn.Module):\n",
    "    def __init__(self, note_dim=768, hidden=256, z_dim=128, num_classes=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.phi_top = nn.Sequential(\n",
    "            nn.Linear(note_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.phi_mid = nn.Sequential(\n",
    "            nn.Linear(note_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.phi_base = nn.Sequential(\n",
    "            nn.Linear(note_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.attn_top = nn.Sequential(\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.attn_mid = nn.Sequential(\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.attn_base = nn.Sequential(\n",
    "            nn.Linear(hidden, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.rho_top = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.rho_mid = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.rho_base = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=4, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden * 6, z_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(z_dim, z_dim), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(z_dim, num_classes)\n",
    "\n",
    "    def aggregate(self, phi, attn_net, rho, notes):\n",
    "        h = phi(notes)\n",
    "        scores = attn_net(h).squeeze(-1)\n",
    "        attn_weights = torch.softmax(scores, dim=1).unsqueeze(-1)\n",
    "        weighted_h = (h * attn_weights).sum(dim=1)\n",
    "        return rho(weighted_h)\n",
    "    \n",
    "    def forward(self, top_notes, mid_notes, base_notes):\n",
    "        h_top = self.aggregate(self.phi_top, self.attn_top, self.rho_top, top_notes)\n",
    "        h_mid = self.aggregate(self.phi_mid, self.attn_mid, self.rho_mid, mid_notes)\n",
    "        h_base = self.aggregate(self.phi_base, self.attn_base, self.rho_base, base_notes)\n",
    "        \n",
    "        h_seq = torch.stack([h_top, h_mid, h_base], dim=1)\n",
    "        h_interact, _ = self.cross_attn(h_seq, h_seq, h_seq)\n",
    "        h_top_i, h_mid_i, h_base_i = h_interact.unbind(dim=1)\n",
    "        \n",
    "        h_all = torch.cat([h_top, h_mid, h_base, h_top_i, h_mid_i, h_base_i], dim=-1)\n",
    "        \n",
    "        z = self.rho(h_all)\n",
    "        logits = self.classifier(z)\n",
    "        return logits, z\n",
    "    \n",
    "pem = PerfumeEmbedding().to(device)\n",
    "state_dict = torch.load(\"models/PEM_v2.pth\")\n",
    "pem.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67199085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get perfume vector\n",
    "def perfume2vec(top_notes, mid_notes, base_notes):\n",
    "    pem.eval()\n",
    "    with torch.no_grad():\n",
    "        top_vecs = get_note_list_vec(top_notes)\n",
    "        mid_vecs = get_note_list_vec(mid_notes)\n",
    "        base_vecs = get_note_list_vec(base_notes)\n",
    "                \n",
    "        top_vecs = torch.tensor(np.array(top_vecs), dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        mid_vecs = torch.tensor(np.array(mid_vecs), dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        base_vecs = torch.tensor(np.array(base_vecs), dtype=torch.float32).to(device).unsqueeze(0)\n",
    "        \n",
    "        _, z = pem(top_vecs, mid_vecs, base_vecs)\n",
    "    return z.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c63bc",
   "metadata": {},
   "source": [
    "# Recommend Perfumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd9b42f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load perfume vector list\n",
    "df_pe = pd.read_csv(\"data/perfume_embedding_v2.csv\")\n",
    "pe_list = df_pe.iloc[:, 1:].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa937c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_perfumes(top_notes, mid_notes, base_notes, top_k=5):\n",
    "    perfume_vec = perfume2vec(top_notes, mid_notes, base_notes).reshape(1, -1)\n",
    "    similarities = cosine_similarity(perfume_vec, pe_list)[0]\n",
    "    df_top = df_pe.assign(similarity=similarities).nlargest(top_k, 'similarity')\n",
    "    recommendation = df_top[['name','similarity']].values.tolist()\n",
    "    return recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98238da9-ef16-4a56-ac12-ab05fb09e130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: 橙子 => 橙子 (similarity: 1.0000)\n",
      "Note: 咖啡 => 咖啡 (similarity: 1.0000)\n",
      "Note: 巧克力 => 巧克力 (similarity: 1.0000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Mugler Angel Fantasm 黑天使幻想女性淡香精', 0.9306738356113713],\n",
       " ['Aquolina Pink Sugar Creamy Sunshine 奶油阳光女性淡香水', 0.9237883703457287],\n",
       " ['Ariana Grande Sweet Like Candy 女性淡香精', 0.9224516461840185],\n",
       " ['Juicy Couture Viva La Juicy Sucre 蛋糕甜心女性淡香精', 0.9219706607464045],\n",
       " ['Juicy Couture Viva La Juicy Sucre 蛋糕甜心女性淡香精 TESTER', 0.9219706607464045],\n",
       " ['Juicy Couture Viva La Juicy Sucre 蛋糕甜心女性淡香精行动香氛', 0.9219706607464045],\n",
       " ['Burberry Goddess 缪斯女神淡香精', 0.9205307121399737],\n",
       " ['Burberry Goddess 缪斯女神淡香精迷你瓶', 0.9096613683790785],\n",
       " ['Aquolina Pink Sugar Red Velvet 红丝绒女性淡香水', 0.9047305913803726],\n",
       " ['Fcuk Friction Her 爱火女性淡香精', 0.9036712784692651]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "like_top_notes = [\"橙子\"]\n",
    "like_mid_notes = [\"咖啡\"]\n",
    "like_base_notes = [\"巧克力\"]\n",
    "top_k = 10\n",
    "\n",
    "rc_perfumes = get_recommended_perfumes(like_top_notes, like_mid_notes, like_base_notes, top_k)\n",
    "display(rc_perfumes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
